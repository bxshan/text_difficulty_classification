{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning based Text Difficulty Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "\n",
    "Right now, there are over 1.5 billion people learning English right now. When learning English, reading English books are an excellent way to improve. Those learners must find English books to read that they can understand and are comfortable with. But it is very difficult to find books that the learner can understand easily, we built a regression model to analyze the text and output the difficulty level of the text. Unlike traditional manual based approaches, this one combine many useful the text features. English learners can use this classification system to better find good books that they can read and will understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility Analysis\n",
    "\n",
    "Logistic regression is, in general a type of generalized linear regression. For this problem, the length of the sentences, the length of the words, and the difficulty of the words are related to the difficulty of the text in a linear pattern. While the relationship is linear, and the output for each text is independent. Hence, we can use a logistic regression model to solve this problem. \n",
    "\n",
    "In this scenario, we can begin by using the above features as input and put the text difficulty level as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "First, we need to find the data set with enough trustworthiness, a clear classification of each text, and is big enough. At last, we decided on books 1-4 of New Concept English. \n",
    "\n",
    "We used the number of each book as a reference to the difficulty. It contains most English grammar, and the data is large enough, with 160 texts. \n",
    "\n",
    "![](img/NCE.png)\n",
    "\n",
    "Most of the code related to data loading functions are in `api.py` , but the main code is in`text_diff.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T03:01:33.809485Z",
     "start_time": "2021-08-29T03:01:33.806928Z"
    }
   },
   "outputs": [],
   "source": [
    "from api.api import *\n",
    "from textstat.textstat import legacy_round, textstatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start to extract the features from the text and classify the difficulty. The **basic features** are the text length, sentence length, word length, and number of sentences. In most cases, the larger those features are, the more difficult the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T03:01:35.516138Z",
     "start_time": "2021-08-29T03:01:35.508456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature: text length\n",
    "def features_len_text(text):\n",
    "    # Normalize to a number between 0..1\n",
    "    len_text = [len(text) / 3000]\n",
    "    return len_text\n",
    "\n",
    "\n",
    "# Feature: sentence length\n",
    "def features_len_sentence(text):\n",
    "    replace_list = [ \n",
    "        '\\n', ':', ';\", '\n",
    "        '', '--', ' $', ' he ', ' him ', ' she ', ' her ', ' the ', '1', '2',\n",
    "        '3', '4', '5', '6', '7', '8', '9', '0', ' they ', ' them '\n",
    "    ]   \n",
    "\n",
    "    for i in replace_list:\n",
    "        text = text.replace(i, ' ')\n",
    "\n",
    "    sentences = re.split('[.?!]', text)\n",
    "\n",
    "    # Ignore all sentences shorter than 5 words\n",
    "    sentences = [i for i in sentences if len(i) >= 5]\n",
    "\n",
    "    num_of_sentence = len(sentences)\n",
    "    len_sentence = [0] * num_of_sentence\n",
    "\n",
    "    words_array = [(i.strip().split(\" \")) for i in sentences]\n",
    "\n",
    "    for i in range(0, num_of_sentence):\n",
    "        for word in words_array[i]:\n",
    "            if len(word) > 1:\n",
    "                len_sentence[i] += 1\n",
    "\n",
    "    # Ignore all sentences where length is bigger than 3-sigma\n",
    "    len_sentence = mean_3_sigma(len_sentence)\n",
    "\n",
    "    # Build a sentence length histogram with 20 buckets\n",
    "    len_sentence_hist = [0] * 20\n",
    "    for i in len_sentence:\n",
    "        len_sentence_hist[min(20, i) % 20] += 1\n",
    "    len_sentence_hist = [i / sum(len_sentence_hist) for i in len_sentence_hist]\n",
    "\n",
    "    return len_sentence_hist\n",
    "\n",
    "\n",
    "# Feature: word length\n",
    "def features_len_word(text):\n",
    "\n",
    "    # Split into single word, and convert every word to its original form, for example \"apples\" to \"apple\"\n",
    "    words = splitwords(text)\n",
    "    len_word = [len(i) for i in words]\n",
    "\n",
    "    # Build a word length histogram with 5 buckets\n",
    "    len_word_hist = [0] * 5\n",
    "    for i in len_word:\n",
    "        len_word_hist[min(21, i) % 5] += 1\n",
    "    len_word_hist = [i / sum(len_word_hist) for i in len_word_hist]\n",
    "\n",
    "    return len_word_hist\n",
    "\n",
    "\n",
    "# Feature: number of sentences\n",
    "def features_num_sentence(text):\n",
    "    sentences = re.split('[.?!]', text)\n",
    "    num_of_sentences = [len(sentences) / 32]\n",
    "\n",
    "    return num_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use more **advanced features**: everybody knows that all English words are made of syllables. The more syllables, the more complicated the word will be. We can use the library syllable_count from textstatistics to calculate the number of syllables. \n",
    "\n",
    "We can also use the number of repeating words to analyze the difficulty of the text. Therefore, if we count the number of unique words, we can get the difficulty of the text.\n",
    "Another advanced feature is word difficulty. The code for it is in api.py. One way is to use the text corpus itself to assign a difficulty level to each word. Another way is to NGSL word frequency table. The higher the frequency, the easier the words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T03:01:39.020161Z",
     "start_time": "2021-08-29T03:01:39.015431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature: syllables per word\n",
    "def features_syllables_word(text):\n",
    "    words = splitwords(text)\n",
    "    syllable_word = [textstatistics().syllable_count(i) for i in words]\n",
    "\n",
    "    # Make a histogram with 5 buckets\n",
    "    syllable_word_hist = [0] * 5\n",
    "    for i in syllable_word:\n",
    "        syllable_word_hist[min(5, i) % 5] += 1\n",
    "    syllable_word_hist = [\n",
    "        i / sum(syllable_word_hist) for i in syllable_word_hist\n",
    "    ]\n",
    "\n",
    "    return syllable_word_hist\n",
    "\n",
    "\n",
    "# Feature: number of unique words\n",
    "def features_unique_words(text):\n",
    "    words_array = splitwords(text)\n",
    "    unique_words = []\n",
    "\n",
    "    # Only count the unique words\n",
    "    for word in words_array:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "    len_unique_words = [len(unique_words) / 230]\n",
    "\n",
    "    return len_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stop with adding more features and focus on how to use these features for model training. In the list \"newFeatures\", every element is a function pointer to the previously defined feature extraction function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T03:01:43.094999Z",
     "start_time": "2021-08-29T03:01:41.445304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Assigning difficulty level to each word\n",
      "2. Generating features from training data\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Assigning difficulty level to each word\")\n",
    "textbooks_data = load_textbooks_data()\n",
    "diff_level = get_diff_level(textbooks_data)\n",
    "\n",
    "print(\"2. Generating features from training data\")\n",
    "newFeatures = [\n",
    "    features_len_text, features_len_sentence,\n",
    "    features_len_word, features_num_sentence, \n",
    "    features_syllables_word, features_unique_words\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the encapsulated logistic regression and order regression functions from SenseTime API to train the model and test it over the testing data. Till now, the relatively basic end to end flow has been established. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T03:02:45.322531Z",
     "start_time": "2021-08-29T03:01:44.728376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Training the regression models, and evaluating the accuracy\n",
      "Start 0 running ...\n",
      "Start 1 running ...\n",
      "Start 2 running ...\n",
      "Start 3 running ...\n",
      "Start 4 running ...\n",
      "Start 5 running ...\n",
      "Start 6 running ...\n",
      "Start 7 running ...\n",
      "Start 8 running ...\n",
      "Start 9 running ...\n",
      "The final model outputs:\n",
      "10x logistic regression model evals:  [0.9, 0.9, 0.8, 0.975, 0.9, 0.875, 0.825, 0.95, 0.8, 0.925]\n",
      "10x order regression model evals:  [0.875, 0.925, 0.825, 0.95, 0.95, 0.825, 0.9, 0.975, 0.875, 0.975]\n",
      "Logic_avg_acc      0.8850000000000001\n",
      "Order_avg_acc      0.9075000000000001\n"
     ]
    }
   ],
   "source": [
    "testtime = 10\n",
    "order_acc_list = []\n",
    "logic_acc_list = []\n",
    "\n",
    "print(\"3. Training the regression models, and evaluating the accuracy\")\n",
    "for i in range(testtime):\n",
    "    print(\"Start\", i, \"running ...\")\n",
    "\n",
    "    shuffle_data(10)\n",
    "\n",
    "    train_data = load_train_data()\n",
    "    train_feats, train_labels = get_feats_labels(train_data,\n",
    "                                                 diff_level,\n",
    "                                                 newFeatures=newFeatures,\n",
    "                                                 diff_use=1)\n",
    "\n",
    "    test_data = load_test_data()\n",
    "    test_feats, test_labels = get_feats_labels(test_data,\n",
    "                                               diff_level,\n",
    "                                               newFeatures=newFeatures,\n",
    "                                               diff_use=1)\n",
    "\n",
    "    # Running the logistic regression model training and evaluation\n",
    "    model = logistic_regression()\n",
    "    model.train(train_feats, train_labels)\n",
    "\n",
    "    pred_y = model.pred(test_feats)\n",
    "    acc = accuracy(pred_y, test_labels)\n",
    "    logic_acc_list.append(acc)\n",
    "\n",
    "    # Running the order regression model training and evaluation\n",
    "    model = order_regression()\n",
    "    model.train(train_feats, train_labels)\n",
    "\n",
    "    pred_y = model.pred(test_feats)\n",
    "    acc = accuracy(pred_y, test_labels)\n",
    "    order_acc_list.append(acc)\n",
    "\n",
    "print(\"The final model outputs:\")\n",
    "print(\"10x logistic regression model evals: \", logic_acc_list)\n",
    "print(\"10x order regression model evals: \", order_acc_list)\n",
    "print(\"Logic_avg_acc     \", sum(logic_acc_list) / testtime)\n",
    "print(\"Order_avg_acc     \", sum(order_acc_list) / testtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "Now, we will explore more approaches to improve the accuracy. \n",
    "#### First, we can add more features. \n",
    "\n",
    "For example, we can use the difficulty level of each sentence based its grammar pattern.\n",
    "\n",
    "To analyze the difficulty of each sentence, we need to analyze the grammar to see if it is easy or hard to understand. We can leverage syntactic parsing to generate a syntactic tree and understand the grammar pattern of the sentence. With the grammar patterns, we can then score each sentence. To achieve this, the toolkit `StanfordDependencyParser` from `nltk` can be a good fit. \n",
    "\n",
    "#### Second, we can tune the parameters. \n",
    "\n",
    "There are two types of parameters: the data related, and the model related. \n",
    "\n",
    "Below are examples of the data related parameters.\n",
    "```python\n",
    "len(unique_words) / 230\n",
    "len(sentences) / 32\n",
    "```\n",
    "In the previous feature extraction code, there are multiple constant numbers which are decided from the text itself. The goal is to make sure the extracted feature numbers will remain between 0 and 1. \n",
    "\n",
    "For the model related parameters, in logistic regression, we can consider these two parameters: \"C\" and \"max_iter\":\n",
    "```python\n",
    "LogisticRegression(C=5000, max_iter=10000)\n",
    "```\n",
    "C is the reciprocal of the regularization parameter. As C increases, the regulation parameter decreases, and the strictness incrases. max_iter is maximum number of iterations taken for the solvers to converge. \n",
    "\n",
    "Through parameter tuning, the accuracy is nearing 90%, which is relative good.\n",
    "\n",
    "#### Third, we can explore other ML models. \n",
    "\n",
    "We learned that order regression can be another good model to use, where we can continue to improve the accuracy further. Besides the regression models, we can also explore other models like RandomForest, XGBoost.\n",
    "\n",
    "At the end of our project, we tested order regression and logistic regression each 10 times. Each time, we reshuffle the data. `shuffle_data` is a function that randomly selects test data among the whole data set. Considering that there are 40 text files for each difficulty level, 10 seems an appropriate ratio. \n",
    "\n",
    "The results are shown below. \n",
    "\n",
    "![](img/result.png)\n",
    "\n",
    "Overall, for this problem, order regression is on average better than logistic regression, achieving a **final accuracy is above 90%**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
